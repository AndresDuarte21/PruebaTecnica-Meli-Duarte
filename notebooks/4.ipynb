{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4d920c",
   "metadata": {},
   "source": [
    "# Sistema de Respuestas Automatizadas para MercadoLibre\n",
    "## An√°lisis Comparativo de Modelos LLM con Sistema de Cach√©\n",
    "\n",
    "Este notebook implementa y eval√∫a un sistema de respuestas automatizadas para preguntas de clientes en MercadoLibre, utilizando diferentes modelos de OpenAI y estrategias de prompting, con un sistema de cach√© basado en embeddings para optimizar costos y tiempo de respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e0f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de embeddings inicializado.\n",
      "\n",
      "‚úÖ Configuraci√≥n inicial completada y lista para inicializar modelos din√°micamente.\n",
      "‚úÖ Funci√≥n de guardado CSV agregada.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "import chromadb\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- Configuraci√≥n de la API Key ---\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"ADVERTENCIA: No se encontr√≥ la API Key de OpenAI. El LLM no funcionar√°.\")\n",
    "\n",
    "# --- Inicializaci√≥n de Embeddings ---\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "print(\"Modelo de embeddings inicializado.\")\n",
    "\n",
    "# --- MODIFICACI√ìN: Funci√≥n para inicializar LLMs din√°micamente ---\n",
    "def inicializar_llm(model_name: str):\n",
    "    \"\"\"Inicializa un modelo de ChatOpenAI con el nombre especificado.\"\"\"\n",
    "    if not api_key:\n",
    "        return None\n",
    "    try:\n",
    "        llm = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            openai_api_key=api_key,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        print(f\"Modelo LLM '{model_name}' inicializado.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error al inicializar el modelo '{model_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "# --- NUEVA FUNCI√ìN: Guardar resultados en CSV ---\n",
    "def guardar_resultados_csv(results_df: pd.DataFrame, modelo: str, prompt_name: str, output_dir: str = \"../results\"):\n",
    "    \"\"\"Guarda los resultados en CSV con el formato especificado: site|pregunta|publicaci√≥n|respuesta generada\"\"\"\n",
    "    \n",
    "    # Crear directorio si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Preparar DataFrame con la estructura requerida\n",
    "    csv_df = pd.DataFrame({\n",
    "        'site': 'MercadoLibre',  # Valor fijo para MercadoLibre\n",
    "        'pregunta': results_df['pregunta'],\n",
    "        'publicacion': results_df['publicacion'],\n",
    "        'respuesta_generada': results_df['respuesta_generada']\n",
    "    })\n",
    "    \n",
    "    # Nombre del archivo\n",
    "    filename = f\"{modelo}_{prompt_name}_resultados.csv\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Guardar con separador |\n",
    "    csv_df.to_csv(filepath, sep='|', index=False, encoding='utf-8-sig')\n",
    "    print(f\"Resultados guardados en: {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "print(\"\\n‚úÖ Configuraci√≥n inicial completada y lista para inicializar modelos din√°micamente.\")\n",
    "print(\"‚úÖ Funci√≥n de guardado CSV agregada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4eb6",
   "metadata": {},
   "source": [
    "## Carga y Exploraci√≥n de Datos\n",
    "\n",
    "Se carga el dataset de preguntas reales de MercadoLibre para evaluar el sistema con casos de uso reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a2d929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado exitosamente desde '..\\data\\preguntas_mercadolibre.xlsx'.\n",
      "\n",
      "El dataset tiene 8 filas y 3 columnas.\n",
      "Primeras filas del dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_22432\\386372061.py:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  file_path = \"..\\data\\preguntas_mercadolibre.xlsx\" # Ajusta la ruta si es necesario\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SITE</th>\n",
       "      <th>PREGUNTA</th>\n",
       "      <th>PUBLICACION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLC</td>\n",
       "      <td>Hola! La mascarilla es de 500 ml, es la origin...</td>\n",
       "      <td>title: Mascarilla Para El Cabello Karseell Col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLM</td>\n",
       "      <td>Hola, es nuevo, facturan, tiene garant√≠a?</td>\n",
       "      <td>title: C√°mara Termogr√°fica Flir Tg297 Point 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLC</td>\n",
       "      <td>Hola, vienen con caja? Son de buena calidad? T...</td>\n",
       "      <td>title: Zapatos Amiu Nuevos De Verano 2024 Para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLB</td>\n",
       "      <td>Voc√™s emitem nota fiscal da compra ?</td>\n",
       "      <td>title: Rodizio Borracha Roda Maci√ßa 250-4 Rol....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLM</td>\n",
       "      <td>Hice compra y quisiera factura</td>\n",
       "      <td>title: Manguera De Jard√≠n Expandible De Hasta ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SITE                                           PREGUNTA  \\\n",
       "0  MLC  Hola! La mascarilla es de 500 ml, es la origin...   \n",
       "1  MLM          Hola, es nuevo, facturan, tiene garant√≠a?   \n",
       "2  MLC  Hola, vienen con caja? Son de buena calidad? T...   \n",
       "3  MLB               Voc√™s emitem nota fiscal da compra ?   \n",
       "4  MLM                     Hice compra y quisiera factura   \n",
       "\n",
       "                                         PUBLICACION  \n",
       "0  title: Mascarilla Para El Cabello Karseell Col...  \n",
       "1  title: C√°mara Termogr√°fica Flir Tg297 Point 19...  \n",
       "2  title: Zapatos Amiu Nuevos De Verano 2024 Para...  \n",
       "3  title: Rodizio Borracha Roda Maci√ßa 250-4 Rol....  \n",
       "4  title: Manguera De Jard√≠n Expandible De Hasta ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Se procesar√° el dataset completo: 8 filas.\n"
     ]
    }
   ],
   "source": [
    "# --- Carga y exploraci√≥n inicial de los datos ---\n",
    "\n",
    "# Ruta al archivo Excel\n",
    "file_path = \"..\\data\\preguntas_mercadolibre.xlsx\" # Ajusta la ruta si es necesario\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(f\"Dataset cargado exitosamente desde '{file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo no se encontr√≥\")\n",
    "\n",
    "# --- Exploraci√≥n b√°sica de los datos ---\n",
    "print(f\"\\nEl dataset tiene {df.shape[0]} filas y {df.shape[1]} columnas.\")\n",
    "print(\"Primeras filas del dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# --- MODIFICACI√ìN: Usaremos el DataFrame completo para el experimento ---\n",
    "df_procesar = df.copy()\n",
    "print(f\"\\nSe procesar√° el dataset completo: {len(df_procesar)} filas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9807369",
   "metadata": {},
   "source": [
    "## Definici√≥n de Estrategias de Prompting\n",
    "\n",
    "Se implementan tres enfoques diferentes para responder preguntas de clientes:\n",
    "\n",
    "1. **Preciso**: Responde √∫nicamente con informaci√≥n disponible en la publicaci√≥n\n",
    "2. **Colaborador**: Prioriza la informaci√≥n de la publicaci√≥n pero puede ofrecer conocimiento general \n",
    "3. **Proactivo**: Enfoque de ventas que destaca caracter√≠sticas positivas del producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c653fb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario de prompts ('preciso', 'colaborador', 'proactivo') creado.\n"
     ]
    }
   ],
   "source": [
    "# --- Diccionario de Plantillas de Prompts ---\n",
    "\n",
    "PROMPTS = {\n",
    "    \"preciso\": \"\"\"\n",
    "**ROL Y OBJETIVO:**\n",
    "Actuar√°s como un asistente virtual experto para Mercado Libre. Tu √∫nica tarea es responder la PREGUNTA DEL CLIENTE usando exclusivamente la INFORMACI√ìN DE LA PUBLICACI√ìN.\n",
    "\n",
    "**REGLAS:**\n",
    "1.  **Fuente √önica:** Basa tu respuesta 100% en la `INFORMACI√ìN DE LA PUBLICACI√ìN`. No inventes ni supongas nada.\n",
    "2.  **Informaci√≥n Ausente:** Si la respuesta no est√° en el texto, tu √∫nica respuesta debe ser: \"¬°Hola! Gracias por tu pregunta. ü§î Esa informaci√≥n no est√° especificada en la publicaci√≥n, ¬°lo siento!\".\n",
    "3.  **Tono:** Amable, conciso y directo. Usa emojis apropiados (üëã, ü§î, üòä).\n",
    "4.  **Idioma:** Responde siempre en el mismo idioma de la pregunta.\n",
    "---\n",
    "**INFORMACI√ìN DE LA PUBLICACI√ìN:**\n",
    "{contexto}\n",
    "\"\"\",\n",
    "\n",
    "    \"colaborador\": \"\"\"\n",
    "**ROL Y OBJETIVO:**\n",
    "Actuar√°s como un asistente virtual colaborativo para Mercado Libre. Tu objetivo es ser lo m√°s √∫til posible, priorizando la INFORMACI√ìN DE LA PUBLICACI√ìN.\n",
    "\n",
    "**REGLAS:**\n",
    "1.  **Prioridad del Contexto:** Busca siempre la respuesta en la `INFORMACI√ìN DE LA PUBLICACI√ìN` primero.\n",
    "2.  **Informaci√≥n Ausente:** Si la respuesta no est√°, adm√≠telo (\"Esa informaci√≥n no est√° detallada en la publicaci√≥n.\") y luego ofrece conocimiento general, indicando que es una suposici√≥n (\"Sin embargo, generalmente en productos de este tipo...\").\n",
    "3.  **Tono:** Servicial, positivo y amable. Usa emojis (‚ú®, üòâ, üëç).\n",
    "4.  **Idioma:** Responde siempre en el mismo idioma de la pregunta.\n",
    "---\n",
    "**INFORMACI√ìN DE LA PUBLICACI√ìN:**\n",
    "{contexto}\n",
    "\"\"\",\n",
    "\n",
    "    \"proactivo\": \"\"\"\n",
    "**ROL Y OBJETIVO:**\n",
    "Actuar√°s como un asistente de ventas proactivo para Mercado Libre. Tu misi√≥n es mantener el inter√©s del cliente y destacar ventajas del producto.\n",
    "\n",
    "**REGLAS:**\n",
    "1.  **Base en el Contexto:** Tus afirmaciones deben originarse en la `INFORMACI√ìN DE LA PUBLICACI√ìN`.\n",
    "2.  **Informaci√≥n Ausente:** Si la respuesta no est√°, no adivines. En su lugar, redirige a una caracter√≠stica positiva que s√≠ est√© en el contexto (\"Ese detalle no est√° especificado, pero ¬°mira esto!, cuenta con [caracter√≠stica positiva]. ü§©\") o sugiere preguntar directamente al vendedor.\n",
    "3.  **Tono:** Entusiasta, positivo y cercano. Usa emojis que generen emoci√≥n (üéâ, ü§©, üöÄ).\n",
    "4.  **Idioma:** Responde siempre en el mismo idioma de la pregunta.\n",
    "---\n",
    "**INFORMACI√ìN DE LA PUBLICACI√ìN:**\n",
    "{contexto}\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Diccionario de prompts ('preciso', 'colaborador', 'proactivo') creado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0967113",
   "metadata": {},
   "source": [
    "## Sistema de Cach√© Basado en Embeddings\n",
    "\n",
    "Implementaci√≥n de un sistema de cach√© inteligente que utiliza ChromaDB y embeddings sem√°nticos para:\n",
    "- Evitar llamadas redundantes a los LLMs\n",
    "- Reducir costos operativos\n",
    "- Mejorar tiempo de respuesta\n",
    "- Reutilizar respuestas para preguntas similares en la misma publicaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a80b874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema de cach√© configurado. Colecci√≥n 'qa_cache_mercadolibre_v3' lista.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuraci√≥n del Cliente y Colecci√≥n de ChromaDB ---\n",
    "client = chromadb.Client()\n",
    "collection_name = \"qa_cache_mercadolibre_v3\" # Usamos v3 para evitar conflictos\n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# --- Contadores y Mapeos para IDs ---\n",
    "publication_to_id_map = {}\n",
    "next_publication_id = 0\n",
    "next_cache_entry_id = 0\n",
    "\n",
    "def get_publication_id(publication_text: str) -> str:\n",
    "    global next_publication_id\n",
    "    if publication_text not in publication_to_id_map:\n",
    "        publication_to_id_map[publication_text] = str(next_publication_id)\n",
    "        next_publication_id += 1\n",
    "    return publication_to_id_map[publication_text]\n",
    "\n",
    "def search_cache(pregunta: str, publication_id: str, similarity_threshold: float = 0.95) -> str | None:\n",
    "    if collection.count() == 0: return None\n",
    "    pregunta_embedding = embedding_model.embed_query(pregunta)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[pregunta_embedding],\n",
    "        n_results=1,\n",
    "        where={\"publication_id\": publication_id}\n",
    "    )\n",
    "    if results['ids'][0]:\n",
    "        similarity = 1 - results['distances'][0][0]\n",
    "        if similarity > similarity_threshold:\n",
    "            return results['metadatas'][0][0]['respuesta']\n",
    "    return None\n",
    "\n",
    "def add_to_cache(pregunta: str, respuesta: str, publication_id: str):\n",
    "    global next_cache_entry_id\n",
    "    pregunta_embedding = embedding_model.embed_query(pregunta)\n",
    "    entry_id = str(next_cache_entry_id)\n",
    "    collection.upsert(\n",
    "        ids=[entry_id],\n",
    "        embeddings=[pregunta_embedding],\n",
    "        metadatas=[{\n",
    "            \"publication_id\": publication_id,\n",
    "            \"pregunta\": pregunta,\n",
    "            \"respuesta\": respuesta\n",
    "        }]\n",
    "    )\n",
    "    next_cache_entry_id += 1\n",
    "\n",
    "# La limpieza de la colecci√≥n se har√° al inicio de cada experimento en el orquestador.\n",
    "print(f\"Sistema de cach√© configurado. Colecci√≥n '{collection_name}' lista.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3f77a",
   "metadata": {},
   "source": [
    "## Generaci√≥n de Respuestas con LLM\n",
    "\n",
    "Funci√≥n principal que utiliza los modelos de OpenAI para generar respuestas contextualmente relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e09d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funci√≥n 'generar_respuesta_llm' actualizada.\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFICACI√ìN: La funci√≥n ahora recibe el objeto LLM ---\n",
    "def generar_respuesta_llm(llm: ChatOpenAI, pregunta: str, publicacion: str, prompt_template: str):\n",
    "    \"\"\"Genera una respuesta utilizando el LLM especificado.\"\"\"\n",
    "    if not llm:\n",
    "        return \"Error: El modelo LLM no fue proporcionado o no est√° inicializado.\"\n",
    "\n",
    "    system_prompt = prompt_template.format(contexto=publicacion)\n",
    "    messages = [SystemMessage(content=system_prompt), HumanMessage(content=pregunta)]\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(messages)\n",
    "        return response.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error al llamar al modelo: {e}\"\n",
    "\n",
    "print(\"Funci√≥n 'generar_respuesta_llm' actualizada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff49da8",
   "metadata": {},
   "source": [
    "## Evaluaci√≥n con RAGAS\n",
    "\n",
    "Implementaci√≥n de evaluaci√≥n autom√°tica usando RAGAS (Retrieval Augmented Generation Assessment) para medir la calidad de las respuestas generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3adbd16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funci√≥n 'evaluar_con_ragas' actualizada.\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFICACI√ìN: La funci√≥n ahora recibe el objeto LLM ---\n",
    "def evaluar_con_ragas(llm: ChatOpenAI, results_df: pd.DataFrame):\n",
    "    \"\"\"Eval√∫a un DataFrame usando RAGAS con el LLM especificado.\"\"\"\n",
    "    if not llm:\n",
    "        print(\"No se puede evaluar con RAGAS porque el LLM no fue proporcionado.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"question\": results_df[\"pregunta\"].tolist(),\n",
    "        \"answer\": results_df[\"respuesta_generada\"].tolist(),\n",
    "        \"contexts\": [[c] for c in results_df[\"publicacion\"].tolist()]\n",
    "    })\n",
    "    \n",
    "    result = evaluate(dataset=dataset, metrics=[faithfulness], llm=llm, embeddings=embedding_model)\n",
    "    return result.to_pandas()\n",
    "\n",
    "print(\"Funci√≥n 'evaluar_con_ragas' actualizada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ace16",
   "metadata": {},
   "source": [
    "## Orquestador de Experimentos\n",
    "\n",
    "Funci√≥n principal que coordina todo el flujo: generaci√≥n de respuestas, uso de cach√©, y evaluaci√≥n de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d4dcb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funci√≥n 'ejecutar_experimento' actualizada para guardar resultados en CSV.\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFICACI√ìN: La funci√≥n ahora recibe el objeto LLM y guarda resultados en CSV ---\n",
    "def ejecutar_experimento(llm: ChatOpenAI, df: pd.DataFrame, prompt_template: str, modelo_name: str = None, prompt_name: str = None):\n",
    "    \"\"\"Orquesta el flujo completo de generaci√≥n de respuestas y guarda resultados en CSV.\"\"\"\n",
    "    results_list = []\n",
    "    \n",
    "    # Reiniciar el cach√© para un experimento limpio\n",
    "    if collection.count() > 0:\n",
    "        collection.delete(ids=collection.get()['ids'])\n",
    "    global next_publication_id, publication_to_id_map, next_cache_entry_id\n",
    "    next_publication_id, next_cache_entry_id = 0, 0\n",
    "    publication_to_id_map = {}\n",
    "    \n",
    "    print(f\"Iniciando experimento para el modelo '{llm.model_name}' con {len(df)} filas...\")\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Generando con {llm.model_name}\"):\n",
    "        pregunta, publicacion = row['PREGUNTA'], row['PUBLICACION']\n",
    "        publication_id = get_publication_id(publicacion)\n",
    "        \n",
    "        cached_response = search_cache(pregunta, publication_id)\n",
    "        \n",
    "        if cached_response:\n",
    "            source, respuesta_final = \"Cache\", cached_response\n",
    "        else:\n",
    "            source = \"LLM\"\n",
    "            respuesta_final = generar_respuesta_llm(llm, pregunta, publicacion, prompt_template)\n",
    "            add_to_cache(pregunta, respuesta_final, publication_id)\n",
    "            time.sleep(1)\n",
    "\n",
    "        results_list.append({\n",
    "            'pregunta': pregunta, 'publicacion': publicacion,\n",
    "            'respuesta_generada': respuesta_final, 'fuente_respuesta': source,\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Guardar resultados en CSV si se proporcionan los nombres\n",
    "    if modelo_name and prompt_name:\n",
    "        guardar_resultados_csv(results_df, modelo_name, prompt_name)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"Funci√≥n 'ejecutar_experimento' actualizada para guardar resultados en CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a9ced",
   "metadata": {},
   "source": [
    "## Benchmark Comparativo de Modelos\n",
    "\n",
    "Evaluaci√≥n sistem√°tica de diferentes combinaciones de modelos GPT y estrategias de prompting para identificar la configuraci√≥n √≥ptima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d566d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LLM 'gpt-4.1-nano' inicializado.\n",
      "Iniciando experimento para el modelo 'gpt-4.1-nano' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6246ceae3594dd6809869e2a4825010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-nano:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-nano_preciso_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e13bbd41584972a6ba407c6c4b1913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1-nano' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784800ebea0f4388a49861b1e907304f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-nano:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-nano_colaborador_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7462a3079167417f80eee5cdd7371c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[7]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1-nano' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3c1c017f584d13a6cdb663e7e28526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-nano:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-nano_proactivo_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b385c8e20fd49eeb419fdebd3548505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LLM 'gpt-4.1-mini' inicializado.\n",
      "Iniciando experimento para el modelo 'gpt-4.1-mini' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bb92e7265f4b818fcc92a7499fe0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-mini:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-mini_preciso_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2142945fa464243acdd32e9becb8673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1-mini' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a166e5af494bf0a6b870ee0911cc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-mini:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-mini_colaborador_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fb463ec18b40efaf244de198021247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1-mini' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96f4a12b57f463bbf23279a32ce2756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-mini:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-mini_proactivo_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70777a961c1a4fb88605171906626509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LLM 'gpt-4.1' inicializado.\n",
      "Iniciando experimento para el modelo 'gpt-4.1' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9a878fccf04831bf9906359b3986ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1_preciso_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e758b783318845eea85d4e410619b903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00ac6edc7664c4b9306b4cd472c5d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1_colaborador_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e0c724edb747298bdfc19ff25cd564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7255f55ac0b7434a8a27d01c7215fa89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1_proactivo_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac7676b993b4dc2a8b9a3639f009481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>prompt</th>\n",
       "      <th>faithfulness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>colaborador</td>\n",
       "      <td>0.7354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>proactivo</td>\n",
       "      <td>0.7335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>proactivo</td>\n",
       "      <td>0.6910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>colaborador</td>\n",
       "      <td>0.6867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>proactivo</td>\n",
       "      <td>0.6486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>colaborador</td>\n",
       "      <td>0.6293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>preciso</td>\n",
       "      <td>0.6129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>preciso</td>\n",
       "      <td>0.5542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>preciso</td>\n",
       "      <td>0.4250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         modelo       prompt  faithfulness_score\n",
       "1  gpt-4.1-nano  colaborador              0.7354\n",
       "5  gpt-4.1-mini    proactivo              0.7335\n",
       "2  gpt-4.1-nano    proactivo              0.6910\n",
       "4  gpt-4.1-mini  colaborador              0.6867\n",
       "8       gpt-4.1    proactivo              0.6486\n",
       "7       gpt-4.1  colaborador              0.6293\n",
       "3  gpt-4.1-mini      preciso              0.6129\n",
       "6       gpt-4.1      preciso              0.5542\n",
       "0  gpt-4.1-nano      preciso              0.4250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. DEFINICI√ìN DEL BENCHMARK ---\n",
    "modelos_a_probar = [\"gpt-4.1-nano\", 'gpt-4.1-mini', 'gpt-4.1']\n",
    "prompts_a_probar = [\"preciso\", \"colaborador\", \"proactivo\"]\n",
    "\n",
    "summary_results = []\n",
    "\n",
    "# --- 2. BUCLE DEL BENCHMARK ---\n",
    "for model_name in modelos_a_probar:\n",
    "    llm = inicializar_llm(model_name)\n",
    "    if not llm: continue\n",
    "\n",
    "    for prompt_name in prompts_a_probar:\n",
    "        template = PROMPTS[prompt_name]\n",
    "        \n",
    "        # Usar la funci√≥n existente ejecutar_experimento con guardado CSV\n",
    "        results_df = ejecutar_experimento(llm, df_procesar, template, model_name, prompt_name)\n",
    "        \n",
    "        llm_results = results_df[results_df['fuente_respuesta'] == 'LLM'].copy()\n",
    "        \n",
    "        if not llm_results.empty:\n",
    "            # Evaluar con RAGAS\n",
    "            ragas_results_df = evaluar_con_ragas(llm, llm_results)\n",
    "            avg_faithfulness = ragas_results_df['faithfulness'].mean() if 'faithfulness' in ragas_results_df.columns else 0.0\n",
    "        else:\n",
    "            avg_faithfulness = 0.0\n",
    "        \n",
    "        summary_results.append({\n",
    "            'modelo': model_name,\n",
    "            'prompt': prompt_name,\n",
    "            'faithfulness_score': round(avg_faithfulness, 4)\n",
    "        })\n",
    "\n",
    "# --- 3. TABLA COMPARATIVA DEL BENCHMARK ---\n",
    "if summary_results:\n",
    "    summary_df = pd.DataFrame(summary_results).sort_values(by='faithfulness_score', ascending=False)\n",
    "    summary_df.to_csv(\"../results/resumen_benchmark.csv\", index=False, encoding='utf-8-sig')\n",
    "    display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca9eb0",
   "metadata": {},
   "source": [
    "## Pruebas del Sistema de Cach√©\n",
    "\n",
    "Validaci√≥n del funcionamiento del sistema de cach√© con datos reales del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cfc7a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funci√≥n 'probar_cache_con_id' definida, que acepta el ID directamente.\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFICACI√ìN: La funci√≥n de prueba ahora recibe el ID de la publicaci√≥n ---\n",
    "\n",
    "def probar_cache_con_id(pregunta: str, publicacion: str, publication_id: str, llm: ChatOpenAI, prompt_template: str):\n",
    "    \"\"\"\n",
    "    Prueba el sistema de cach√© usando directamente el ID de la publicaci√≥n.\n",
    "\n",
    "    Args:\n",
    "        pregunta (str): La pregunta del cliente.\n",
    "        publicacion (str): El texto de la publicaci√≥n (necesario para el LLM si hay un cache miss).\n",
    "        publication_id (str): El ID √∫nico de la publicaci√≥n.\n",
    "        llm (ChatOpenAI): El objeto LLM para generar respuestas.\n",
    "        prompt_template (str): La plantilla de prompt.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str]: Una tupla con la respuesta y la fuente ('Cache' o 'LLM').\n",
    "    \"\"\"\n",
    "    # 1. Buscar en el cach√© usando el ID\n",
    "    cached_response = search_cache(pregunta, publication_id)\n",
    "    \n",
    "    if cached_response:\n",
    "        # Cache Hit\n",
    "        return cached_response, \"Cache\"\n",
    "    else:\n",
    "        # Cache Miss\n",
    "        # 2. Generar respuesta si no est√° en el cach√©\n",
    "        nueva_respuesta = generar_respuesta_llm(llm, pregunta, publicacion, prompt_template)\n",
    "        # 3. Guardar en el cach√© usando el ID\n",
    "        add_to_cache(pregunta, nueva_respuesta, publication_id)\n",
    "        return nueva_respuesta, \"LLM\"\n",
    "\n",
    "print(\"Funci√≥n 'probar_cache_con_id' definida, que acepta el ID directamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando datos reales del DataFrame 'df_procesar' para la prueba.\n",
      "\n",
      "Datos de prueba extra√≠dos de la primera fila del dataset:\n",
      "  - Pregunta: 'Hola! La mascarilla es de 500 ml, es la original? Y tiene alg√∫n contacto donde le pueda realizar algunas preguntas'\n",
      "\n",
      "ID obtenido para la publicaci√≥n: '0'\n",
      "\n",
      "--- PRIMERA LLAMADA ---\n",
      "Fuente: Cache\n",
      "Respuesta: ¬°Hola! ü§© S√≠, la mascarilla para el cabello Karseell Collagen que ves en la publicaci√≥n es de 500 ml, tal como lo indica el t√≠tulo. Sobre si es la original, ese detalle espec√≠fico no est√° mencionado en la informaci√≥n de la publicaci√≥n, pero ¬°mira esto! Es un producto nuevo, viene en presentaci√≥n individual y cuenta con garant√≠a del vendedor por 60 d√≠as, lo que te da mayor confianza en tu compra. üéâ\n",
      "\n",
      "Respecto a un contacto directo, Mercado Libre recomienda que todas las consultas se realicen a trav√©s de la secci√≥n de preguntas de la publicaci√≥n, ¬°as√≠ el vendedor puede responderte de forma segura y r√°pida! Si tienes m√°s dudas, ¬°aqu√≠ estoy para ayudarte! üöÄ\n",
      "\n",
      "--- SEGUNDA LLAMADA ---\n",
      "Fuente: LLM\n",
      "Respuesta: ¬°Hola! Gracias por tu pregunta. ü§î Esa informaci√≥n no est√° especificada en la publicaci√≥n, ¬°lo siento!\n",
      "\n",
      "--- TERCERA LLAMADA ---\n",
      "Fuente: Cache\n",
      "Respuesta: ¬°Hola! Gracias por tu pregunta. ü§î Esa informaci√≥n no est√° especificada en la publicaci√≥n, ¬°lo siento!\n"
     ]
    }
   ],
   "source": [
    "print(\"Usando datos reales del DataFrame 'df_procesar' para la prueba.\")\n",
    "\n",
    "\n",
    "fila_de_prueba = df_procesar.iloc[0]\n",
    "pregunta_real = fila_de_prueba['PREGUNTA']\n",
    "publicacion_real = fila_de_prueba['PUBLICACION']\n",
    "\n",
    "print(\"\\nDatos de prueba extra√≠dos de la primera fila del dataset:\")\n",
    "print(f\"  - Pregunta: '{pregunta_real}'\")\n",
    "\n",
    "id_publicacion_real = get_publication_id(publicacion_real)\n",
    "print(f\"\\nID obtenido para la publicaci√≥n: '{id_publicacion_real}'\")\n",
    "\n",
    "prompt = PROMPTS['preciso'] \n",
    "\n",
    "print(\"\\n--- PRIMERA LLAMADA ---\")\n",
    "respuesta1, fuente1 = probar_cache_con_id(pregunta_real, publicacion_real, id_publicacion_real, llm, prompt)\n",
    "print(f\"Fuente: {fuente1}\")\n",
    "print(f\"Respuesta: {respuesta1}\")\n",
    "\n",
    "print(\"\\n--- SEGUNDA LLAMADA ---\")\n",
    "respuesta2, fuente2 = probar_cache_con_id(\"¬øQu√© otros productos tienes?\", publicacion_real, id_publicacion_real, llm, prompt)\n",
    "print(f\"Fuente: {fuente2}\")\n",
    "print(f\"Respuesta: {respuesta2}\")\n",
    "\n",
    "print(\"\\n--- TERCERA LLAMADA ---\")\n",
    "respuesta3, fuente3 = probar_cache_con_id(\"¬øQu√© otros productos tienes?\", publicacion_real, id_publicacion_real, llm, prompt)\n",
    "print(f\"Fuente: {fuente3}\")\n",
    "print(f\"Respuesta: {respuesta3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127371b9",
   "metadata": {},
   "source": [
    "El sistema de cach√© est√° pensado para evitar redundancia sobre la misma pregunta. La intuici√≥n me dice que es muy probable que una gran proporci√≥n de las preguntas que se reciben sean repetitivas y fundamentalmente iguales, por lo que este sistema tiene sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3422da0",
   "metadata": {},
   "source": [
    "# Conclusiones y Hallazgos\n",
    "\n",
    "## Sistema de Cach√© Basado en Embeddings\n",
    "\n",
    "Dado el gran volumen de preguntas que recibe MercadoLibre semanalmente, tiene sentido econ√≥mico implementar un sistema de cach√© inteligente. El sistema propuesto utiliza embeddings sem√°nticos para identificar preguntas similares sobre la misma publicaci√≥n y reutilizar respuestas previamente generadas por LLMs, optimizando tanto costos como tiempo de respuesta.\n",
    "\n",
    "## Selecci√≥n de Modelo √ìptimo (Corregido)\n",
    "\n",
    "Basado en los resultados del benchmark de faithfulness score:\n",
    "\n",
    "**Recomendaci√≥n**: **GPT-4.1-nano con prompt \"colaborador\"** (0.7354)\n",
    "- Mayor fidelidad a la informaci√≥n de la publicaci√≥n.\n",
    "- Reduce el riesgo de alucinaciones.\n",
    "- Modelo m√°s econ√≥mico (\"nano\") con el mejor rendimiento.\n",
    "\n",
    "**Alternativa de Alto Rendimiento**: **GPT-4.1-mini con prompt \"proactivo\"** (0.7335)\n",
    "- Performance casi id√©ntica al modelo recomendado.\n",
    "- Excelente balance entre calidad y costo.\n",
    "- Ideal si se busca una respuesta m√°s proactiva sin sacrificar fidelidad.\n",
    "\n",
    "## An√°lisis de Costos\n",
    "\n",
    "**Precios de inferencia por 1M tokens:**\n",
    "- GPT-4.1: Input $3.00, Output $12.00\n",
    "- GPT-4.1-mini: Input $0.80, Output $3.20  \n",
    "- GPT-4.1-nano: Input $0.20, Output $0.80\n",
    "\n",
    "El sistema de cach√© puede reducir de manera significativa las llamadas a LLM para preguntas frecuentes, generando ahorros sustanciales. Sin embargo, debe considerarse el costo de hospedar la base de datos vectorial (ChromaDB/Pinecone) versus los ahorros en tokens de LLM.\n",
    "\n",
    "## Evaluaci√≥n con RAGAS\n",
    "\n",
    "Se implement√≥ RAGAS para establecer un est√°ndar de evaluaci√≥n objetiva. Se utiliz√≥ la m√©trica \"faithfulness\" por simplicidad, pero existen m√∫ltiples m√©tricas adicionales (answer_relevancy, context_precision, etc.) que deber√≠an alinearse con objetivos espec√≠ficos del negocio.\n",
    "\n",
    "Es importante destacar que esta es una forma automatizada de evaluar LLMs con otros sistemas de LLMs, que tambi√©n est√°n sujetos a variaciones y a incertidumbre. La definici√≥n de una m√©trica adecuada suele ir acompa√±ada de una alineaci√≥n previa con negocio. \n",
    "\n",
    "Por otro lado, existen muchas otras formas de evaluar el rendimiento de un LLM en este tipo de tareas. Eleg√≠ la anterior por simplicidad.\n",
    "\n",
    "## Archivos de Resultados Generados\n",
    "\n",
    "El sistema ahora genera autom√°ticamente archivos CSV con la estructura solicitada:\n",
    "- **Formato**: `site|pregunta|publicacion|respuesta_generada`\n",
    "- **Ubicaci√≥n**: `../results/`\n",
    "- **Nomenclatura**: `{modelo}_{prompt}_resultados.csv`\n",
    "\n",
    "Cada combinaci√≥n de modelo y prompt genera un archivo CSV independiente para facilitar el an√°lisis posterior.\n",
    "\n",
    "## Pr√≥ximos Pasos\n",
    "\n",
    "### Gesti√≥n de Problemas Identificados:\n",
    "\n",
    "1. **Alucinaciones**: \n",
    "   - Validaci√≥n autom√°tica de respuestas contra contexto de publicaci√≥n.\n",
    "   - Evaluaci√≥n continua antes de la respuesta. \n",
    "   - Tama√±o del modelo.\n",
    "\n",
    "2. **Informaci√≥n Contradictoria**:\n",
    "   - Sistema de versionado de publicaciones en el cach√©\n",
    "   - Invalidaci√≥n autom√°tica cuando se actualiza una publicaci√≥n para el sistema de Cach√©. \n",
    "\n",
    "3. **Ausencia de Datos**:\n",
    "   - Respuestas predefinidas que admiten limitaciones: \"Esa informaci√≥n no est√° especificada\"\n",
    "   - Redirecci√≥n a contacto directo con vendedor o con un agente especializado.\n",
    "   - Incentivo a completar la descripci√≥n de forma oportuna. \n",
    "\n",
    "4. **Optimizaci√≥n del Sistema**:\n",
    "   - Implementar m√©tricas de negocio espec√≠ficas (conversi√≥n, satisfacci√≥n)\n",
    "   - A/B testing entre diferentes estrategias de prompting y percepci√≥n de un usuario final.\n",
    "   - Monitoreo continuo de calidad y costos.\n",
    "\n",
    "La soluci√≥n propuesta representa un enfoque balanceado entre calidad de respuestas, eficiencia operativa y control. Sin embargo, tiene muchas oportunidades de mejora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
