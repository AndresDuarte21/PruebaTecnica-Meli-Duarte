{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4d920c",
   "metadata": {},
   "source": [
    "# Sistema de Respuestas Automatizadas para MercadoLibre\n",
    "## Análisis Comparativo de Modelos LLM con Sistema de Caché\n",
    "\n",
    "Este notebook implementa y evalúa un sistema de respuestas automatizadas para preguntas de clientes en MercadoLibre, utilizando diferentes modelos de OpenAI y estrategias de prompting, con un sistema de caché basado en embeddings para optimizar costos y tiempo de respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e0f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de embeddings inicializado.\n",
      "\n",
      "✅ Configuración inicial completada y lista para inicializar modelos dinámicamente.\n",
      "✅ Función de guardado CSV agregada.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "import chromadb\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- Configuración de la API Key ---\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"ADVERTENCIA: No se encontró la API Key de OpenAI. El LLM no funcionará.\")\n",
    "\n",
    "# --- Inicialización de Embeddings ---\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "print(\"Modelo de embeddings inicializado.\")\n",
    "\n",
    "# --- MODIFICACIÓN: Función para inicializar LLMs dinámicamente ---\n",
    "def inicializar_llm(model_name: str):\n",
    "    \"\"\"Inicializa un modelo de ChatOpenAI con el nombre especificado.\"\"\"\n",
    "    if not api_key:\n",
    "        return None\n",
    "    try:\n",
    "        llm = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            openai_api_key=api_key,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        print(f\"Modelo LLM '{model_name}' inicializado.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error al inicializar el modelo '{model_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "# --- NUEVA FUNCIÓN: Guardar resultados en CSV ---\n",
    "def guardar_resultados_csv(results_df: pd.DataFrame, modelo: str, prompt_name: str, output_dir: str = \"../results\"):\n",
    "    \"\"\"Guarda los resultados en CSV con el formato especificado: site|pregunta|publicación|respuesta generada\"\"\"\n",
    "    \n",
    "    # Crear directorio si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Preparar DataFrame con la estructura requerida\n",
    "    csv_df = pd.DataFrame({\n",
    "        'site': 'MercadoLibre',  # Valor fijo para MercadoLibre\n",
    "        'pregunta': results_df['pregunta'],\n",
    "        'publicacion': results_df['publicacion'],\n",
    "        'respuesta_generada': results_df['respuesta_generada']\n",
    "    })\n",
    "    \n",
    "    # Nombre del archivo\n",
    "    filename = f\"{modelo}_{prompt_name}_resultados.csv\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Guardar con separador |\n",
    "    csv_df.to_csv(filepath, sep='|', index=False, encoding='utf-8-sig')\n",
    "    print(f\"Resultados guardados en: {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "print(\"\\n✅ Configuración inicial completada y lista para inicializar modelos dinámicamente.\")\n",
    "print(\"✅ Función de guardado CSV agregada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4eb6",
   "metadata": {},
   "source": [
    "## Carga y Exploración de Datos\n",
    "\n",
    "Se carga el dataset de preguntas reales de MercadoLibre para evaluar el sistema con casos de uso reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a2d929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado exitosamente desde '..\\data\\preguntas_mercadolibre.xlsx'.\n",
      "\n",
      "El dataset tiene 8 filas y 3 columnas.\n",
      "Primeras filas del dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_22432\\386372061.py:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  file_path = \"..\\data\\preguntas_mercadolibre.xlsx\" # Ajusta la ruta si es necesario\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SITE</th>\n",
       "      <th>PREGUNTA</th>\n",
       "      <th>PUBLICACION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLC</td>\n",
       "      <td>Hola! La mascarilla es de 500 ml, es la origin...</td>\n",
       "      <td>title: Mascarilla Para El Cabello Karseell Col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLM</td>\n",
       "      <td>Hola, es nuevo, facturan, tiene garantía?</td>\n",
       "      <td>title: Cámara Termográfica Flir Tg297 Point 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLC</td>\n",
       "      <td>Hola, vienen con caja? Son de buena calidad? T...</td>\n",
       "      <td>title: Zapatos Amiu Nuevos De Verano 2024 Para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLB</td>\n",
       "      <td>Vocês emitem nota fiscal da compra ?</td>\n",
       "      <td>title: Rodizio Borracha Roda Maciça 250-4 Rol....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLM</td>\n",
       "      <td>Hice compra y quisiera factura</td>\n",
       "      <td>title: Manguera De Jardín Expandible De Hasta ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SITE                                           PREGUNTA  \\\n",
       "0  MLC  Hola! La mascarilla es de 500 ml, es la origin...   \n",
       "1  MLM          Hola, es nuevo, facturan, tiene garantía?   \n",
       "2  MLC  Hola, vienen con caja? Son de buena calidad? T...   \n",
       "3  MLB               Vocês emitem nota fiscal da compra ?   \n",
       "4  MLM                     Hice compra y quisiera factura   \n",
       "\n",
       "                                         PUBLICACION  \n",
       "0  title: Mascarilla Para El Cabello Karseell Col...  \n",
       "1  title: Cámara Termográfica Flir Tg297 Point 19...  \n",
       "2  title: Zapatos Amiu Nuevos De Verano 2024 Para...  \n",
       "3  title: Rodizio Borracha Roda Maciça 250-4 Rol....  \n",
       "4  title: Manguera De Jardín Expandible De Hasta ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Se procesará el dataset completo: 8 filas.\n"
     ]
    }
   ],
   "source": [
    "# --- Carga y exploración inicial de los datos ---\n",
    "\n",
    "# Ruta al archivo Excel\n",
    "file_path = \"..\\data\\preguntas_mercadolibre.xlsx\" # Ajusta la ruta si es necesario\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(f\"Dataset cargado exitosamente desde '{file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo no se encontró\")\n",
    "\n",
    "# --- Exploración básica de los datos ---\n",
    "print(f\"\\nEl dataset tiene {df.shape[0]} filas y {df.shape[1]} columnas.\")\n",
    "print(\"Primeras filas del dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# --- MODIFICACIÓN: Usaremos el DataFrame completo para el experimento ---\n",
    "df_procesar = df.copy()\n",
    "print(f\"\\nSe procesará el dataset completo: {len(df_procesar)} filas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9807369",
   "metadata": {},
   "source": [
    "## Definición de Estrategias de Prompting\n",
    "\n",
    "Se implementan tres enfoques diferentes para responder preguntas de clientes:\n",
    "\n",
    "1. **Preciso**: Responde únicamente con información disponible en la publicación\n",
    "2. **Colaborador**: Prioriza la información de la publicación pero puede ofrecer conocimiento general \n",
    "3. **Proactivo**: Enfoque de ventas que destaca características positivas del producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c653fb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario de prompts ('preciso', 'colaborador', 'proactivo') creado.\n"
     ]
    }
   ],
   "source": [
    "# --- Diccionario de Plantillas de Prompts ---\n",
    "\n",
    "PROMPTS = {\n",
    "    \"preciso\": \"\"\"\n",
    "**ROL Y OBJETIVO:**\n",
    "Actuarás como un asistente virtual experto para Mercado Libre. Tu única tarea es responder la PREGUNTA DEL CLIENTE usando exclusivamente la INFORMACIÓN DE LA PUBLICACIÓN.\n",
    "\n",
    "**REGLAS:**\n",
    "1.  **Fuente Única:** Basa tu respuesta 100% en la `INFORMACIÓN DE LA PUBLICACIÓN`. No inventes ni supongas nada.\n",
    "2.  **Información Ausente:** Si la respuesta no está en el texto, tu única respuesta debe ser: \"¡Hola! Gracias por tu pregunta. 🤔 Esa información no está especificada en la publicación, ¡lo siento!\".\n",
    "3.  **Tono:** Amable, conciso y directo. Usa emojis apropiados (👋, 🤔, 😊).\n",
    "4.  **Idioma:** Responde siempre en el mismo idioma de la pregunta.\n",
    "---\n",
    "**INFORMACIÓN DE LA PUBLICACIÓN:**\n",
    "{contexto}\n",
    "\"\"\",\n",
    "\n",
    "    \"colaborador\": \"\"\"\n",
    "**ROL Y OBJETIVO:**\n",
    "Actuarás como un asistente virtual colaborativo para Mercado Libre. Tu objetivo es ser lo más útil posible, priorizando la INFORMACIÓN DE LA PUBLICACIÓN.\n",
    "\n",
    "**REGLAS:**\n",
    "1.  **Prioridad del Contexto:** Busca siempre la respuesta en la `INFORMACIÓN DE LA PUBLICACIÓN` primero.\n",
    "2.  **Información Ausente:** Si la respuesta no está, admítelo (\"Esa información no está detallada en la publicación.\") y luego ofrece conocimiento general, indicando que es una suposición (\"Sin embargo, generalmente en productos de este tipo...\").\n",
    "3.  **Tono:** Servicial, positivo y amable. Usa emojis (✨, 😉, 👍).\n",
    "4.  **Idioma:** Responde siempre en el mismo idioma de la pregunta.\n",
    "---\n",
    "**INFORMACIÓN DE LA PUBLICACIÓN:**\n",
    "{contexto}\n",
    "\"\"\",\n",
    "\n",
    "    \"proactivo\": \"\"\"\n",
    "**ROL Y OBJETIVO:**\n",
    "Actuarás como un asistente de ventas proactivo para Mercado Libre. Tu misión es mantener el interés del cliente y destacar ventajas del producto.\n",
    "\n",
    "**REGLAS:**\n",
    "1.  **Base en el Contexto:** Tus afirmaciones deben originarse en la `INFORMACIÓN DE LA PUBLICACIÓN`.\n",
    "2.  **Información Ausente:** Si la respuesta no está, no adivines. En su lugar, redirige a una característica positiva que sí esté en el contexto (\"Ese detalle no está especificado, pero ¡mira esto!, cuenta con [característica positiva]. 🤩\") o sugiere preguntar directamente al vendedor.\n",
    "3.  **Tono:** Entusiasta, positivo y cercano. Usa emojis que generen emoción (🎉, 🤩, 🚀).\n",
    "4.  **Idioma:** Responde siempre en el mismo idioma de la pregunta.\n",
    "---\n",
    "**INFORMACIÓN DE LA PUBLICACIÓN:**\n",
    "{contexto}\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Diccionario de prompts ('preciso', 'colaborador', 'proactivo') creado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0967113",
   "metadata": {},
   "source": [
    "## Sistema de Caché Basado en Embeddings\n",
    "\n",
    "Implementación de un sistema de caché inteligente que utiliza ChromaDB y embeddings semánticos para:\n",
    "- Evitar llamadas redundantes a los LLMs\n",
    "- Reducir costos operativos\n",
    "- Mejorar tiempo de respuesta\n",
    "- Reutilizar respuestas para preguntas similares en la misma publicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a80b874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema de caché configurado. Colección 'qa_cache_mercadolibre_v3' lista.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuración del Cliente y Colección de ChromaDB ---\n",
    "client = chromadb.Client()\n",
    "collection_name = \"qa_cache_mercadolibre_v3\" # Usamos v3 para evitar conflictos\n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# --- Contadores y Mapeos para IDs ---\n",
    "publication_to_id_map = {}\n",
    "next_publication_id = 0\n",
    "next_cache_entry_id = 0\n",
    "\n",
    "def get_publication_id(publication_text: str) -> str:\n",
    "    global next_publication_id\n",
    "    if publication_text not in publication_to_id_map:\n",
    "        publication_to_id_map[publication_text] = str(next_publication_id)\n",
    "        next_publication_id += 1\n",
    "    return publication_to_id_map[publication_text]\n",
    "\n",
    "def search_cache(pregunta: str, publication_id: str, similarity_threshold: float = 0.95) -> str | None:\n",
    "    if collection.count() == 0: return None\n",
    "    pregunta_embedding = embedding_model.embed_query(pregunta)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[pregunta_embedding],\n",
    "        n_results=1,\n",
    "        where={\"publication_id\": publication_id}\n",
    "    )\n",
    "    if results['ids'][0]:\n",
    "        similarity = 1 - results['distances'][0][0]\n",
    "        if similarity > similarity_threshold:\n",
    "            return results['metadatas'][0][0]['respuesta']\n",
    "    return None\n",
    "\n",
    "def add_to_cache(pregunta: str, respuesta: str, publication_id: str):\n",
    "    global next_cache_entry_id\n",
    "    pregunta_embedding = embedding_model.embed_query(pregunta)\n",
    "    entry_id = str(next_cache_entry_id)\n",
    "    collection.upsert(\n",
    "        ids=[entry_id],\n",
    "        embeddings=[pregunta_embedding],\n",
    "        metadatas=[{\n",
    "            \"publication_id\": publication_id,\n",
    "            \"pregunta\": pregunta,\n",
    "            \"respuesta\": respuesta\n",
    "        }]\n",
    "    )\n",
    "    next_cache_entry_id += 1\n",
    "\n",
    "# La limpieza de la colección se hará al inicio de cada experimento en el orquestador.\n",
    "print(f\"Sistema de caché configurado. Colección '{collection_name}' lista.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3f77a",
   "metadata": {},
   "source": [
    "## Generación de Respuestas con LLM\n",
    "\n",
    "Función principal que utiliza los modelos de OpenAI para generar respuestas contextualmente relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e09d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función 'generar_respuesta_llm' actualizada.\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFICACIÓN: La función ahora recibe el objeto LLM ---\n",
    "def generar_respuesta_llm(llm: ChatOpenAI, pregunta: str, publicacion: str, prompt_template: str):\n",
    "    \"\"\"Genera una respuesta utilizando el LLM especificado.\"\"\"\n",
    "    if not llm:\n",
    "        return \"Error: El modelo LLM no fue proporcionado o no está inicializado.\"\n",
    "\n",
    "    system_prompt = prompt_template.format(contexto=publicacion)\n",
    "    messages = [SystemMessage(content=system_prompt), HumanMessage(content=pregunta)]\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(messages)\n",
    "        return response.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error al llamar al modelo: {e}\"\n",
    "\n",
    "print(\"Función 'generar_respuesta_llm' actualizada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff49da8",
   "metadata": {},
   "source": [
    "## Evaluación con RAGAS\n",
    "\n",
    "Implementación de evaluación automática usando RAGAS (Retrieval Augmented Generation Assessment) para medir la calidad de las respuestas generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3adbd16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función 'evaluar_con_ragas' actualizada.\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFICACIÓN: La función ahora recibe el objeto LLM ---\n",
    "def evaluar_con_ragas(llm: ChatOpenAI, results_df: pd.DataFrame):\n",
    "    \"\"\"Evalúa un DataFrame usando RAGAS con el LLM especificado.\"\"\"\n",
    "    if not llm:\n",
    "        print(\"No se puede evaluar con RAGAS porque el LLM no fue proporcionado.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"question\": results_df[\"pregunta\"].tolist(),\n",
    "        \"answer\": results_df[\"respuesta_generada\"].tolist(),\n",
    "        \"contexts\": [[c] for c in results_df[\"publicacion\"].tolist()]\n",
    "    })\n",
    "    \n",
    "    result = evaluate(dataset=dataset, metrics=[faithfulness], llm=llm, embeddings=embedding_model)\n",
    "    return result.to_pandas()\n",
    "\n",
    "print(\"Función 'evaluar_con_ragas' actualizada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ace16",
   "metadata": {},
   "source": [
    "## Orquestador de Experimentos\n",
    "\n",
    "Función principal que coordina todo el flujo: generación de respuestas, uso de caché, y evaluación de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d4dcb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función 'ejecutar_experimento' actualizada para guardar resultados en CSV.\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFICACIÓN: La función ahora recibe el objeto LLM y guarda resultados en CSV ---\n",
    "def ejecutar_experimento(llm: ChatOpenAI, df: pd.DataFrame, prompt_template: str, modelo_name: str = None, prompt_name: str = None):\n",
    "    \"\"\"Orquesta el flujo completo de generación de respuestas y guarda resultados en CSV.\"\"\"\n",
    "    results_list = []\n",
    "    \n",
    "    # Reiniciar el caché para un experimento limpio\n",
    "    if collection.count() > 0:\n",
    "        collection.delete(ids=collection.get()['ids'])\n",
    "    global next_publication_id, publication_to_id_map, next_cache_entry_id\n",
    "    next_publication_id, next_cache_entry_id = 0, 0\n",
    "    publication_to_id_map = {}\n",
    "    \n",
    "    print(f\"Iniciando experimento para el modelo '{llm.model_name}' con {len(df)} filas...\")\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Generando con {llm.model_name}\"):\n",
    "        pregunta, publicacion = row['PREGUNTA'], row['PUBLICACION']\n",
    "        publication_id = get_publication_id(publicacion)\n",
    "        \n",
    "        cached_response = search_cache(pregunta, publication_id)\n",
    "        \n",
    "        if cached_response:\n",
    "            source, respuesta_final = \"Cache\", cached_response\n",
    "        else:\n",
    "            source = \"LLM\"\n",
    "            respuesta_final = generar_respuesta_llm(llm, pregunta, publicacion, prompt_template)\n",
    "            add_to_cache(pregunta, respuesta_final, publication_id)\n",
    "            time.sleep(1)\n",
    "\n",
    "        results_list.append({\n",
    "            'pregunta': pregunta, 'publicacion': publicacion,\n",
    "            'respuesta_generada': respuesta_final, 'fuente_respuesta': source,\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Guardar resultados en CSV si se proporcionan los nombres\n",
    "    if modelo_name and prompt_name:\n",
    "        guardar_resultados_csv(results_df, modelo_name, prompt_name)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"Función 'ejecutar_experimento' actualizada para guardar resultados en CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a9ced",
   "metadata": {},
   "source": [
    "## Benchmark Comparativo de Modelos\n",
    "\n",
    "Evaluación sistemática de diferentes combinaciones de modelos GPT y estrategias de prompting para identificar la configuración óptima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d566d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LLM 'gpt-4.1-nano' inicializado.\n",
      "Iniciando experimento para el modelo 'gpt-4.1-nano' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6246ceae3594dd6809869e2a4825010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-nano:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-nano_preciso_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e13bbd41584972a6ba407c6c4b1913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1-nano' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784800ebea0f4388a49861b1e907304f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-nano:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-nano_colaborador_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7462a3079167417f80eee5cdd7371c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[7]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1-nano' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3c1c017f584d13a6cdb663e7e28526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-nano:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-nano_proactivo_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b385c8e20fd49eeb419fdebd3548505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LLM 'gpt-4.1-mini' inicializado.\n",
      "Iniciando experimento para el modelo 'gpt-4.1-mini' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bb92e7265f4b818fcc92a7499fe0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-mini:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-mini_preciso_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2142945fa464243acdd32e9becb8673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1-mini' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a166e5af494bf0a6b870ee0911cc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-mini:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-mini_colaborador_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fb463ec18b40efaf244de198021247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1-mini' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96f4a12b57f463bbf23279a32ce2756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1-mini:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1-mini_proactivo_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70777a961c1a4fb88605171906626509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LLM 'gpt-4.1' inicializado.\n",
      "Iniciando experimento para el modelo 'gpt-4.1' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9a878fccf04831bf9906359b3986ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1_preciso_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e758b783318845eea85d4e410619b903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00ac6edc7664c4b9306b4cd472c5d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1_colaborador_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e0c724edb747298bdfc19ff25cd564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento para el modelo 'gpt-4.1' con 8 filas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7255f55ac0b7434a8a27d01c7215fa89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generando con gpt-4.1:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: ../results\\gpt-4.1_proactivo_resultados.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac7676b993b4dc2a8b9a3639f009481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>prompt</th>\n",
       "      <th>faithfulness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>colaborador</td>\n",
       "      <td>0.7354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>proactivo</td>\n",
       "      <td>0.7335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>proactivo</td>\n",
       "      <td>0.6910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>colaborador</td>\n",
       "      <td>0.6867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>proactivo</td>\n",
       "      <td>0.6486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>colaborador</td>\n",
       "      <td>0.6293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>preciso</td>\n",
       "      <td>0.6129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>preciso</td>\n",
       "      <td>0.5542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>preciso</td>\n",
       "      <td>0.4250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         modelo       prompt  faithfulness_score\n",
       "1  gpt-4.1-nano  colaborador              0.7354\n",
       "5  gpt-4.1-mini    proactivo              0.7335\n",
       "2  gpt-4.1-nano    proactivo              0.6910\n",
       "4  gpt-4.1-mini  colaborador              0.6867\n",
       "8       gpt-4.1    proactivo              0.6486\n",
       "7       gpt-4.1  colaborador              0.6293\n",
       "3  gpt-4.1-mini      preciso              0.6129\n",
       "6       gpt-4.1      preciso              0.5542\n",
       "0  gpt-4.1-nano      preciso              0.4250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. DEFINICIÓN DEL BENCHMARK ---\n",
    "modelos_a_probar = [\"gpt-4.1-nano\", 'gpt-4.1-mini', 'gpt-4.1']\n",
    "prompts_a_probar = [\"preciso\", \"colaborador\", \"proactivo\"]\n",
    "\n",
    "summary_results = []\n",
    "\n",
    "# --- 2. BUCLE DEL BENCHMARK ---\n",
    "for model_name in modelos_a_probar:\n",
    "    llm = inicializar_llm(model_name)\n",
    "    if not llm: continue\n",
    "\n",
    "    for prompt_name in prompts_a_probar:\n",
    "        template = PROMPTS[prompt_name]\n",
    "        \n",
    "        # Usar la función existente ejecutar_experimento con guardado CSV\n",
    "        results_df = ejecutar_experimento(llm, df_procesar, template, model_name, prompt_name)\n",
    "        \n",
    "        llm_results = results_df[results_df['fuente_respuesta'] == 'LLM'].copy()\n",
    "        \n",
    "        if not llm_results.empty:\n",
    "            # Evaluar con RAGAS\n",
    "            ragas_results_df = evaluar_con_ragas(llm, llm_results)\n",
    "            avg_faithfulness = ragas_results_df['faithfulness'].mean() if 'faithfulness' in ragas_results_df.columns else 0.0\n",
    "        else:\n",
    "            avg_faithfulness = 0.0\n",
    "        \n",
    "        summary_results.append({\n",
    "            'modelo': model_name,\n",
    "            'prompt': prompt_name,\n",
    "            'faithfulness_score': round(avg_faithfulness, 4)\n",
    "        })\n",
    "\n",
    "# --- 3. TABLA COMPARATIVA DEL BENCHMARK ---\n",
    "if summary_results:\n",
    "    summary_df = pd.DataFrame(summary_results).sort_values(by='faithfulness_score', ascending=False)\n",
    "    summary_df.to_csv(\"../results/resumen_benchmark.csv\", index=False, encoding='utf-8-sig')\n",
    "    display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca9eb0",
   "metadata": {},
   "source": [
    "## Pruebas del Sistema de Caché\n",
    "\n",
    "Validación del funcionamiento del sistema de caché con datos reales del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cfc7a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función 'probar_cache_con_id' definida, que acepta el ID directamente.\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFICACIÓN: La función de prueba ahora recibe el ID de la publicación ---\n",
    "\n",
    "def probar_cache_con_id(pregunta: str, publicacion: str, publication_id: str, llm: ChatOpenAI, prompt_template: str):\n",
    "    \"\"\"\n",
    "    Prueba el sistema de caché usando directamente el ID de la publicación.\n",
    "\n",
    "    Args:\n",
    "        pregunta (str): La pregunta del cliente.\n",
    "        publicacion (str): El texto de la publicación (necesario para el LLM si hay un cache miss).\n",
    "        publication_id (str): El ID único de la publicación.\n",
    "        llm (ChatOpenAI): El objeto LLM para generar respuestas.\n",
    "        prompt_template (str): La plantilla de prompt.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str]: Una tupla con la respuesta y la fuente ('Cache' o 'LLM').\n",
    "    \"\"\"\n",
    "    # 1. Buscar en el caché usando el ID\n",
    "    cached_response = search_cache(pregunta, publication_id)\n",
    "    \n",
    "    if cached_response:\n",
    "        # Cache Hit\n",
    "        return cached_response, \"Cache\"\n",
    "    else:\n",
    "        # Cache Miss\n",
    "        # 2. Generar respuesta si no está en el caché\n",
    "        nueva_respuesta = generar_respuesta_llm(llm, pregunta, publicacion, prompt_template)\n",
    "        # 3. Guardar en el caché usando el ID\n",
    "        add_to_cache(pregunta, nueva_respuesta, publication_id)\n",
    "        return nueva_respuesta, \"LLM\"\n",
    "\n",
    "print(\"Función 'probar_cache_con_id' definida, que acepta el ID directamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando datos reales del DataFrame 'df_procesar' para la prueba.\n",
      "\n",
      "Datos de prueba extraídos de la primera fila del dataset:\n",
      "  - Pregunta: 'Hola! La mascarilla es de 500 ml, es la original? Y tiene algún contacto donde le pueda realizar algunas preguntas'\n",
      "\n",
      "ID obtenido para la publicación: '0'\n",
      "\n",
      "--- PRIMERA LLAMADA ---\n",
      "Fuente: Cache\n",
      "Respuesta: ¡Hola! 🤩 Sí, la mascarilla para el cabello Karseell Collagen que ves en la publicación es de 500 ml, tal como lo indica el título. Sobre si es la original, ese detalle específico no está mencionado en la información de la publicación, pero ¡mira esto! Es un producto nuevo, viene en presentación individual y cuenta con garantía del vendedor por 60 días, lo que te da mayor confianza en tu compra. 🎉\n",
      "\n",
      "Respecto a un contacto directo, Mercado Libre recomienda que todas las consultas se realicen a través de la sección de preguntas de la publicación, ¡así el vendedor puede responderte de forma segura y rápida! Si tienes más dudas, ¡aquí estoy para ayudarte! 🚀\n",
      "\n",
      "--- SEGUNDA LLAMADA ---\n",
      "Fuente: LLM\n",
      "Respuesta: ¡Hola! Gracias por tu pregunta. 🤔 Esa información no está especificada en la publicación, ¡lo siento!\n",
      "\n",
      "--- TERCERA LLAMADA ---\n",
      "Fuente: Cache\n",
      "Respuesta: ¡Hola! Gracias por tu pregunta. 🤔 Esa información no está especificada en la publicación, ¡lo siento!\n"
     ]
    }
   ],
   "source": [
    "print(\"Usando datos reales del DataFrame 'df_procesar' para la prueba.\")\n",
    "\n",
    "\n",
    "fila_de_prueba = df_procesar.iloc[0]\n",
    "pregunta_real = fila_de_prueba['PREGUNTA']\n",
    "publicacion_real = fila_de_prueba['PUBLICACION']\n",
    "\n",
    "print(\"\\nDatos de prueba extraídos de la primera fila del dataset:\")\n",
    "print(f\"  - Pregunta: '{pregunta_real}'\")\n",
    "\n",
    "id_publicacion_real = get_publication_id(publicacion_real)\n",
    "print(f\"\\nID obtenido para la publicación: '{id_publicacion_real}'\")\n",
    "\n",
    "prompt = PROMPTS['preciso'] \n",
    "\n",
    "print(\"\\n--- PRIMERA LLAMADA ---\")\n",
    "respuesta1, fuente1 = probar_cache_con_id(pregunta_real, publicacion_real, id_publicacion_real, llm, prompt)\n",
    "print(f\"Fuente: {fuente1}\")\n",
    "print(f\"Respuesta: {respuesta1}\")\n",
    "\n",
    "print(\"\\n--- SEGUNDA LLAMADA ---\")\n",
    "respuesta2, fuente2 = probar_cache_con_id(\"¿Qué otros productos tienes?\", publicacion_real, id_publicacion_real, llm, prompt)\n",
    "print(f\"Fuente: {fuente2}\")\n",
    "print(f\"Respuesta: {respuesta2}\")\n",
    "\n",
    "print(\"\\n--- TERCERA LLAMADA ---\")\n",
    "respuesta3, fuente3 = probar_cache_con_id(\"¿Qué otros productos tienes?\", publicacion_real, id_publicacion_real, llm, prompt)\n",
    "print(f\"Fuente: {fuente3}\")\n",
    "print(f\"Respuesta: {respuesta3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127371b9",
   "metadata": {},
   "source": [
    "El sistema de caché está pensado para evitar redundancia sobre la misma pregunta. La intuición me dice que es muy probable que una gran proporción de las preguntas que se reciben sean repetitivas y fundamentalmente iguales, por lo que este sistema tiene sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3422da0",
   "metadata": {},
   "source": [
    "# Conclusiones y Hallazgos\n",
    "\n",
    "## Sistema de Caché Basado en Embeddings\n",
    "\n",
    "Dado el gran volumen de preguntas que recibe MercadoLibre semanalmente, tiene sentido económico implementar un sistema de caché inteligente. El sistema propuesto utiliza embeddings semánticos para identificar preguntas similares sobre la misma publicación y reutilizar respuestas previamente generadas por LLMs, optimizando tanto costos como tiempo de respuesta.\n",
    "\n",
    "## Selección de Modelo Óptimo (Corregido)\n",
    "\n",
    "Basado en los resultados del benchmark de faithfulness score:\n",
    "\n",
    "**Recomendación**: **GPT-4.1-nano con prompt \"colaborador\"** (0.7354)\n",
    "- Mayor fidelidad a la información de la publicación.\n",
    "- Reduce el riesgo de alucinaciones.\n",
    "- Modelo más económico (\"nano\") con el mejor rendimiento.\n",
    "\n",
    "**Alternativa de Alto Rendimiento**: **GPT-4.1-mini con prompt \"proactivo\"** (0.7335)\n",
    "- Performance casi idéntica al modelo recomendado.\n",
    "- Excelente balance entre calidad y costo.\n",
    "- Ideal si se busca una respuesta más proactiva sin sacrificar fidelidad.\n",
    "\n",
    "## Análisis de Costos\n",
    "\n",
    "**Precios de inferencia por 1M tokens:**\n",
    "- GPT-4.1: Input $3.00, Output $12.00\n",
    "- GPT-4.1-mini: Input $0.80, Output $3.20  \n",
    "- GPT-4.1-nano: Input $0.20, Output $0.80\n",
    "\n",
    "El sistema de caché puede reducir de manera significativa las llamadas a LLM para preguntas frecuentes, generando ahorros sustanciales. Sin embargo, debe considerarse el costo de hospedar la base de datos vectorial (ChromaDB/Pinecone) versus los ahorros en tokens de LLM.\n",
    "\n",
    "## Evaluación con RAGAS\n",
    "\n",
    "Se implementó RAGAS para establecer un estándar de evaluación objetiva. Se utilizó la métrica \"faithfulness\" por simplicidad, pero existen múltiples métricas adicionales (answer_relevancy, context_precision, etc.) que deberían alinearse con objetivos específicos del negocio.\n",
    "\n",
    "Es importante destacar que esta es una forma automatizada de evaluar LLMs con otros sistemas de LLMs, que también están sujetos a variaciones y a incertidumbre. La definición de una métrica adecuada suele ir acompañada de una alineación previa con negocio. \n",
    "\n",
    "Por otro lado, existen muchas otras formas de evaluar el rendimiento de un LLM en este tipo de tareas. Elegí la anterior por simplicidad.\n",
    "\n",
    "## Archivos de Resultados Generados\n",
    "\n",
    "El sistema ahora genera automáticamente archivos CSV con la estructura solicitada:\n",
    "- **Formato**: `site|pregunta|publicacion|respuesta_generada`\n",
    "- **Ubicación**: `../results/`\n",
    "- **Nomenclatura**: `{modelo}_{prompt}_resultados.csv`\n",
    "\n",
    "Cada combinación de modelo y prompt genera un archivo CSV independiente para facilitar el análisis posterior.\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "### Gestión de Problemas Identificados:\n",
    "\n",
    "1. **Alucinaciones**: \n",
    "   - Validación automática de respuestas contra contexto de publicación.\n",
    "   - Evaluación continua antes de la respuesta. \n",
    "   - Tamaño del modelo.\n",
    "\n",
    "2. **Información Contradictoria**:\n",
    "   - Sistema de versionado de publicaciones en el caché\n",
    "   - Invalidación automática cuando se actualiza una publicación para el sistema de Caché. \n",
    "\n",
    "3. **Ausencia de Datos**:\n",
    "   - Respuestas predefinidas que admiten limitaciones: \"Esa información no está especificada\"\n",
    "   - Redirección a contacto directo con vendedor o con un agente especializado.\n",
    "   - Incentivo a completar la descripción de forma oportuna. \n",
    "\n",
    "4. **Optimización del Sistema**:\n",
    "   - Implementar métricas de negocio específicas (conversión, satisfacción)\n",
    "   - A/B testing entre diferentes estrategias de prompting y percepción de un usuario final.\n",
    "   - Monitoreo continuo de calidad y costos.\n",
    "\n",
    "La solución propuesta representa un enfoque balanceado entre calidad de respuestas, eficiencia operativa y control. Sin embargo, tiene muchas oportunidades de mejora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
